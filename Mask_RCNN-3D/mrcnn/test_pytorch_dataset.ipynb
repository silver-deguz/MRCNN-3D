{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random \n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import utils\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn\n",
      "/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/shapes/train\n",
      "/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/shapes/val\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.dirname(os.path.realpath('__file__')))\n",
    "root_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "print(root_dir)\n",
    "\n",
    "exp_name = 'shapes'\n",
    "image_height = image_width = 320\n",
    "train_dir = os.path.join(root_dir, exp_name, 'train')\n",
    "val_dir = os.path.join(root_dir, exp_name, 'val')\n",
    "print(train_dir)\n",
    "print(val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes Dataset from Matterport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dir):\n",
    "        super(ShapesDataset, self).__init__()\n",
    "        self.out_dir = out_dir\n",
    "        if not os.path.exists(self.out_dir):\n",
    "            os.makedirs(self.out_dir)\n",
    "    \n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "            \n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes\n",
    "           \n",
    "    def save_image_and_mask(self, image_id):\n",
    "        img = self.load_image(image_id)\n",
    "        seg, class_id = self.load_mask(image_id)\n",
    "        out = np.concatenate((img, seg), axis=2)\n",
    "        out_path = os.path.join(self.out_dir, '{}.npy'.format(image_id))\n",
    "        self.image_info[image_id]['path'] = out_path\n",
    "        np.save(out_path, out)\n",
    "        \n",
    "        with open(os.path.join(self.out_dir, 'meta_info_{}.pickle'.format(image_id)), 'wb') as handle:\n",
    "            pickle.dump([out_path, class_id, str(image_id)], handle)\n",
    "        \n",
    "def aggregate_meta_info(exp_dir):\n",
    "    files = [os.path.join(exp_dir, f) for f in os.listdir(exp_dir) if 'meta_info' in f]\n",
    "    df = pd.DataFrame(columns=['path', 'class_id', 'pid'])\n",
    "    \n",
    "    for f in files:\n",
    "        with open(f, 'rb') as handle:\n",
    "            df.loc[len(df)] = pickle.load(handle)\n",
    "    df.to_pickle(os.path.join(exp_dir, 'info_df.pickle'))\n",
    "    print (\"aggregated meta info to df with length\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset(train_dir)\n",
    "dataset_train.load_shapes(500, image_height, image_width)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset(val_dir)\n",
    "dataset_val.load_shapes(50, image_height, image_width)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 320, 3)\n",
      "(320, 320, 3)\n",
      "{'id': 300, 'source': 'shapes', 'path': None, 'width': 320, 'height': 320, 'bg_color': array([151, 230, 249]), 'shapes': [('triangle', (181, 84, 60), (97, 46, 50)), ('circle', (24, 46, 238), (115, 251, 47)), ('circle', (147, 95, 181), (263, 60, 56))]} \n",
      "\n",
      "(320, 320, 3)\n",
      "(320, 320, 3)\n",
      "{'id': 330, 'source': 'shapes', 'path': None, 'width': 320, 'height': 320, 'bg_color': array([185,  68, 132]), 'shapes': [('circle', (255, 68, 42), (263, 41, 78)), ('circle', (128, 120, 22), (291, 151, 61)), ('square', (25, 247, 219), (264, 253, 55))]} \n",
      "\n",
      "(320, 320, 3)\n",
      "(320, 320, 1)\n",
      "{'id': 124, 'source': 'shapes', 'path': None, 'width': 320, 'height': 320, 'bg_color': array([165, 150,  23]), 'shapes': [('triangle', (145, 114, 70), (276, 265, 73))]} \n",
      "\n",
      "(320, 320, 3)\n",
      "(320, 320, 1)\n",
      "{'id': 322, 'source': 'shapes', 'path': None, 'width': 320, 'height': 320, 'bg_color': array([164, 156, 140]), 'shapes': [('square', (98, 220, 215), (190, 57, 56))]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print(image.shape)\n",
    "    print(mask.shape)\n",
    "    print(dataset_train.image_info[image_id],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 300, 'source': 'shapes', 'path': '/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/shapes/train/300.npy', 'width': 320, 'height': 320, 'bg_color': array([151, 230, 249]), 'shapes': [('triangle', (181, 84, 60), (97, 46, 50)), ('circle', (24, 46, 238), (115, 251, 47)), ('circle', (147, 95, 181), (263, 60, 56))]}\n",
      "{'id': 330, 'source': 'shapes', 'path': '/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/shapes/train/330.npy', 'width': 320, 'height': 320, 'bg_color': array([185,  68, 132]), 'shapes': [('circle', (255, 68, 42), (263, 41, 78)), ('circle', (128, 120, 22), (291, 151, 61)), ('square', (25, 247, 219), (264, 253, 55))]}\n",
      "{'id': 124, 'source': 'shapes', 'path': '/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/shapes/train/124.npy', 'width': 320, 'height': 320, 'bg_color': array([165, 150,  23]), 'shapes': [('triangle', (145, 114, 70), (276, 265, 73))]}\n",
      "{'id': 322, 'source': 'shapes', 'path': '/Users/jdeguzman/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/shapes/train/322.npy', 'width': 320, 'height': 320, 'bg_color': array([164, 156, 140]), 'shapes': [('square', (98, 220, 215), (190, 57, 56))]}\n"
     ]
    }
   ],
   "source": [
    "for image_id in image_ids:\n",
    "    dataset_train.save_image_and_mask(image_id)\n",
    "    print(dataset_train.image_info[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"execution script.\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# import utils.exp_utils as utils\n",
    "import utils\n",
    "from evaluator import Evaluator\n",
    "from predictor import Predictor\n",
    "from plotting import plot_batch_prediction\n",
    "\n",
    "def load_image_gt(dataset, config, image_id, augment=False, use_mini_mask=False):\n",
    "    \"\"\"Load and return ground truth data for an image (image, mask, bounding boxes).\n",
    "    \n",
    "    augment: If true, apply random image augmentation. Currently, only horizontal \n",
    "        flipping is offered.\n",
    "    use_mini_mask: If False, returns full-size masks that are the same height\n",
    "        and width as the original image. These can be big, for example\n",
    "        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,\n",
    "        224x224 and are generated by extracting the bounding box of the\n",
    "        object and resizing it to MINI_MASK_SHAPE.\n",
    "    \n",
    "    Returns:\n",
    "    image: [height, width, 3]\n",
    "    shape: the original shape of the image before resizing and cropping.\n",
    "    class_ids: [instance_count] Integer class IDs\n",
    "    bbox: [instance_count, (y1, x1, y2, x2)]\n",
    "    mask: [height, width, instance_count]. The height and width are those\n",
    "        of the image unless use_mini_mask is True, in which case they are\n",
    "        defined in MINI_MASK_SHAPE.\n",
    "    \"\"\"\n",
    "    # Load image and mask\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    original_shape = image.shape\n",
    "    image, window, scale, padding, crop = utils.resize_image(\n",
    "        image,\n",
    "        min_dim=config.IMAGE_MIN_DIM,\n",
    "        min_scale=config.IMAGE_MIN_SCALE,\n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        mode=config.IMAGE_RESIZE_MODE)\n",
    "    mask = utils.resize_mask(mask, scale, padding, crop)\n",
    "\n",
    "    # Random horizontal flips.\n",
    "    if augment:\n",
    "        logging.warning(\"'augment' is deprecated. Use 'augmentation' instead.\")\n",
    "        if random.randint(0, 1):\n",
    "            image = np.fliplr(image)\n",
    "            mask = np.fliplr(mask)\n",
    "\n",
    "    # Note that some boxes might be all zeros if the corresponding mask got cropped out.\n",
    "    # and here is to filter them out\n",
    "    _idx = np.sum(mask, axis=(0, 1)) > 0\n",
    "    mask = mask[:, :, _idx]\n",
    "    class_ids = class_ids[_idx]\n",
    "    \n",
    "    # Bounding boxes. Note that some boxes might be all zeros\n",
    "    # if the corresponding mask got cropped out.\n",
    "    # bbox: [num_instances, (y1, x1, y2, x2)]\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "    # Active classes\n",
    "    # Different datasets have different classes, so track the\n",
    "    # classes supported in the dataset of this image.\n",
    "    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\n",
    "    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\"source\"]]\n",
    "    active_class_ids[source_class_ids] = 1\n",
    "\n",
    "    # Resize masks to smaller size to reduce memory usage\n",
    "    if use_mini_mask:\n",
    "        mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
    "\n",
    "    # Image meta data\n",
    "#     image_meta = compose_image_meta(image_id, original_shape, image.shape,\n",
    "#                                     window, scale, active_class_ids)\n",
    "\n",
    "    return image, class_ids, bbox, mask\n",
    "\n",
    "def train(logger, dataset_train):\n",
    "    \"\"\"\n",
    "    perform the training routine for a given fold. saves plots and selected parameters to the experiment dir\n",
    "    specified in the configs.\n",
    "    \"\"\"\n",
    "    logger.info('performing training in {}D over fold {} on experiment {} with model {}'.format(\n",
    "        cf.dim, cf.fold, cf.exp_dir, cf.model))\n",
    "\n",
    "    net = model.net(cf, logger).cuda()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=cf.learning_rate[0], weight_decay=cf.weight_decay)\n",
    "    model_selector = utils.ModelSelector(cf, logger)\n",
    "    train_evaluator = Evaluator(cf, logger, mode='train')\n",
    "    val_evaluator = Evaluator(cf, logger, mode=cf.val_mode)\n",
    "\n",
    "    starting_epoch = 1\n",
    "\n",
    "    # prepare monitoring\n",
    "    monitor_metrics, TrainingPlot = utils.prepare_monitoring(cf)\n",
    "\n",
    "    if cf.resume_to_checkpoint:\n",
    "        starting_epoch, monitor_metrics = utils.load_checkpoint(cf.resume_to_checkpoint, net, optimizer)\n",
    "        logger.info('resumed to checkpoint {} at epoch {}'.format(cf.resume_to_checkpoint, starting_epoch))\n",
    "\n",
    "    logger.info('loading dataset and initializing batch generators...')\n",
    "#     batch_gen = data_loader.get_train_generators(cf, logger)\n",
    "    \n",
    "    \n",
    "    for epoch in range(starting_epoch, cf.num_epochs + 1):\n",
    "        logger.info('starting training epoch {}'.format(epoch))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = cf.learning_rate[epoch - 1]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        net.train() # function built into nn.module\n",
    "        train_results_list = []\n",
    "        num_train_batches = len(dataset_train.image_ids)\n",
    "        for bix in range(num_train_batches):\n",
    "#             batch = next(batch_gen['train'])\n",
    "            batch = {}\n",
    "            image, class_id, bbox, mask = load_image_gt(dataset_train, config, bix)\n",
    "            \n",
    "            batch['data'] = image\n",
    "            batch['roi_labels'] = class_id\n",
    "            batch['bb_target'] = bbox\n",
    "            batch['roi_masks'] = mask\n",
    "            batch['pid'] = dataset_train.image_ids[0]\n",
    "            \n",
    "            tic_fw = time.time()\n",
    "            results_dict = net.train_forward(batch) ### TODO: ensure this works with mods\n",
    "            tic_bw = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            results_dict['torch_loss'].backward()\n",
    "            optimizer.step()\n",
    "            logger.info('tr. batch {0}/{1} (ep. {2}) fw {3:.3f}s / bw {4:.3f}s / total {5:.3f}s || '\n",
    "                        .format(bix + 1, num_train_batches, epoch, tic_bw - tic_fw,\n",
    "                                time.time() - tic_bw, time.time() - tic_fw) + results_dict['logger_string'])\n",
    "            train_results_list.append([results_dict['boxes'], batch['pid']])\n",
    "            monitor_metrics['train']['monitor_values'][epoch].append(results_dict['monitor_values'])\n",
    "\n",
    "        _, monitor_metrics['train'] = train_evaluator.evaluate_predictions(train_results_list, monitor_metrics['train'])\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        logger.info('starting validation in mode {}.'.format(cf.val_mode))\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            if cf.do_validation:\n",
    "                val_results_list = []\n",
    "                val_predictor = Predictor(cf, net, logger, mode='val')\n",
    "                for _ in range(batch_gen['n_val']):\n",
    "                    batch = next(batch_gen[cf.val_mode])\n",
    "                    if cf.val_mode == 'val_patient':\n",
    "                        results_dict = val_predictor.predict_patient(batch)\n",
    "                    elif cf.val_mode == 'val_sampling':\n",
    "                        results_dict = net.train_forward(batch, is_validation=True)\n",
    "                    val_results_list.append([results_dict['boxes'], batch['pid']])\n",
    "                    monitor_metrics['val']['monitor_values'][epoch].append(results_dict['monitor_values'])\n",
    "\n",
    "                _, monitor_metrics['val'] = val_evaluator.evaluate_predictions(val_results_list, monitor_metrics['val'])\n",
    "                model_selector.run_model_selection(net, optimizer, monitor_metrics, epoch)\n",
    "\n",
    "            # update monitoring and prediction plots\n",
    "            TrainingPlot.update_and_save(monitor_metrics, epoch)\n",
    "            epoch_time = time.time() - start_time\n",
    "            logger.info('trained epoch {}: took {} sec. ({} train / {} val)'.format(\n",
    "                epoch, epoch_time, train_time, epoch_time-train_time))\n",
    "            batch = next(batch_gen['val_sampling'])\n",
    "            results_dict = net.train_forward(batch, is_validation=True)\n",
    "            logger.info('plotting predictions from validation sampling.')\n",
    "            plot_batch_prediction(batch, results_dict, cf)\n",
    "\n",
    "\n",
    "# def test(logger):\n",
    "#     \"\"\"\n",
    "#     perform testing for a given fold (or hold out set). save stats in evaluator.\n",
    "#     \"\"\"\n",
    "#     logger.info('starting testing model of fold {} in exp {}'.format(cf.fold, cf.exp_dir))\n",
    "#     net = model.net(cf, logger).cuda()\n",
    "#     test_predictor = Predictor(cf, net, logger, mode='test')\n",
    "#     test_evaluator = Evaluator(cf, logger, mode='test')\n",
    "#     batch_gen = data_loader.get_test_generator(cf, logger)\n",
    "#     test_results_list = test_predictor.predict_test_set(batch_gen, return_results=True)\n",
    "#     test_evaluator.evaluate_predictions(test_results_list)\n",
    "#     test_evaluator.score_test_df()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--mode', type=str,  default='train_test',\n",
    "                        help='one out of: train / test / train_test / analysis / create_exp')\n",
    "    parser.add_argument('--folds', nargs='+', type=int, default=None,\n",
    "                        help='None runs over all folds in CV. otherwise specify list of folds.')\n",
    "    parser.add_argument('--exp_dir', type=str, default='/path/to/experiment/directory',\n",
    "                        help='path to experiment dir. will be created if non existent.')\n",
    "    parser.add_argument('--server_env', default=False, action='store_true',\n",
    "                        help='change IO settings to deploy models on a cluster.')\n",
    "    parser.add_argument('--slurm_job_id', type=str, default=None, help='job scheduler info')\n",
    "    parser.add_argument('--use_stored_settings', default=False, action='store_true',\n",
    "                        help='load configs from existing exp_dir instead of source dir. always done for testing, '\n",
    "                             'but can be set to true to do the same for training. useful in job scheduler environment, '\n",
    "                             'where source code might change before the job actually runs.')\n",
    "    parser.add_argument('--resume_to_checkpoint', type=str, default=None,\n",
    "                        help='if resuming to checkpoint, the desired fold still needs to be parsed via --folds.')\n",
    "    parser.add_argument('--exp_source', type=str, default='experiments/toy_exp',\n",
    "                        help='specifies, from which source experiment to load configs and data_loader.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    folds = args.folds\n",
    "    resume_to_checkpoint = args.resume_to_checkpoint\n",
    "\n",
    "    if args.mode == 'train' or args.mode == 'train_test':\n",
    "\n",
    "        cf = utils.prep_exp(args.exp_source, args.exp_dir, args.server_env, args.use_stored_settings)\n",
    "        cf.slurm_job_id = args.slurm_job_id\n",
    "        model = utils.import_module('model', cf.model_path)\n",
    "        data_loader = utils.import_module('dl', os.path.join(args.exp_source, 'data_loader.py'))\n",
    "        if folds is None:\n",
    "            folds = range(cf.n_cv_splits)\n",
    "\n",
    "        for fold in folds:\n",
    "            cf.fold_dir = os.path.join(cf.exp_dir, 'fold_{}'.format(fold))\n",
    "            cf.fold = fold\n",
    "            cf.resume_to_checkpoint = resume_to_checkpoint\n",
    "            if not os.path.exists(cf.fold_dir):\n",
    "                os.mkdir(cf.fold_dir)\n",
    "            logger = utils.get_logger(cf.fold_dir)\n",
    "            train(logger)\n",
    "            cf.resume_to_checkpoint = None\n",
    "            if args.mode == 'train_test':\n",
    "                test(logger)\n",
    "\n",
    "    elif args.mode == 'test':\n",
    "\n",
    "        cf = utils.prep_exp(args.exp_source, args.exp_dir, args.server_env, is_training=False, use_stored_settings=True)\n",
    "        cf.slurm_job_id = args.slurm_job_id\n",
    "        model = utils.import_module('model', cf.model_path)\n",
    "        data_loader = utils.import_module('dl', os.path.join(args.exp_source, 'data_loader.py'))\n",
    "        if folds is None:\n",
    "            folds = range(cf.n_cv_splits)\n",
    "\n",
    "        for fold in folds:\n",
    "            cf.fold_dir = os.path.join(cf.exp_dir, 'fold_{}'.format(fold))\n",
    "            logger = utils.get_logger(cf.fold_dir)\n",
    "            cf.fold = fold\n",
    "            test(logger)\n",
    "\n",
    "    # load raw predictions saved by predictor during testing, run aggregation algorithms and evaluation.\n",
    "    elif args.mode == 'analysis':\n",
    "        cf = utils.prep_exp(args.exp_source, args.exp_dir, args.server_env, is_training=False, use_stored_settings=True)\n",
    "        logger = utils.get_logger(cf.exp_dir)\n",
    "\n",
    "        if cf.hold_out_test_set:\n",
    "            cf.folds = args.folds\n",
    "            predictor = Predictor(cf, net=None, logger=logger, mode='analysis')\n",
    "            results_list = predictor.load_saved_predictions(apply_wbc=True)\n",
    "            utils.create_csv_output(results_list, cf, logger)\n",
    "\n",
    "        else:\n",
    "            if folds is None:\n",
    "                folds = range(cf.n_cv_splits)\n",
    "            for fold in folds:\n",
    "                cf.fold_dir = os.path.join(cf.exp_dir, 'fold_{}'.format(fold))\n",
    "                cf.fold = fold\n",
    "                predictor = Predictor(cf, net=None, logger=logger, mode='analysis')\n",
    "                results_list = predictor.load_saved_predictions(apply_wbc=True)\n",
    "                logger.info('starting evaluation...')\n",
    "                evaluator = Evaluator(cf, logger, mode='test')\n",
    "                evaluator.evaluate_predictions(results_list)\n",
    "                evaluator.score_test_df()\n",
    "\n",
    "    # create experiment folder and copy scripts without starting job.\n",
    "    # usefull for cloud deployment where configs might change before job actually runs.\n",
    "    elif args.mode == 'create_exp':\n",
    "        cf = utils.prep_exp(args.exp_source, args.exp_dir, args.server_env, use_stored_settings=True)\n",
    "        logger = utils.get_logger(cf.exp_dir)\n",
    "        logger.info('created experiment directory at {}'.format(args.exp_dir))\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError('mode specified in args is not implemented...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-43e0a86c8c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#  Mask R-CNN Class\n",
    "############################################################\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self, cf, logger):\n",
    "\n",
    "        super(net, self).__init__()\n",
    "        self.cf = cf\n",
    "        self.logger = logger\n",
    "        self.build()\n",
    "\n",
    "        if self.cf.weight_init is not None:\n",
    "            logger.info(\"using pytorch weight init of type {}\".format(self.cf.weight_init))\n",
    "            mutils.initialize_weights(self)\n",
    "        else:\n",
    "            logger.info(\"using default pytorch weight init\")\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Build Mask R-CNN architecture.\"\"\"\n",
    "\n",
    "        # Image size must be dividable by 2 multiple times.\n",
    "        h, w = self.cf.patch_size[:2]\n",
    "        if h / 2**5 != int(h / 2**5) or w / 2**5 != int(w / 2**5):\n",
    "            raise Exception(\"Image size must be dividable by 2 at least 5 times \"\n",
    "                            \"to avoid fractions when downscaling and upscaling.\"\n",
    "                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "        if len(self.cf.patch_size) == 3:\n",
    "            d = self.cf.patch_size[2]\n",
    "            if d / 2**3 != int(d / 2**3):\n",
    "                raise Exception(\"Image z dimension must be dividable by 2 at least 3 times \"\n",
    "                                \"to avoid fractions when downscaling and upscaling.\")\n",
    "\n",
    "\n",
    "\n",
    "        # instanciate abstract multi dimensional conv class and backbone class.\n",
    "        conv = mutils.NDConvGenerator(self.cf.dim)\n",
    "        backbone = utils.import_module('bbone', self.cf.backbone_path)\n",
    "\n",
    "        # build Anchors, FPN, RPN, Classifier / Bbox-Regressor -head, Mask-head\n",
    "        self.np_anchors = mutils.generate_pyramid_anchors(self.logger, self.cf)\n",
    "        self.anchors = torch.from_numpy(self.np_anchors).float().cuda()\n",
    "        self.fpn = backbone.FPN(self.cf, conv)\n",
    "        self.rpn = RPN(self.cf, conv)\n",
    "        self.classifier = Classifier(self.cf, conv)\n",
    "        self.mask = Mask(self.cf, conv)\n",
    "\n",
    "\n",
    "    def train_forward(self, batch, is_validation=False):\n",
    "        \"\"\"\n",
    "        train method (also used for validation monitoring). wrapper around forward pass of network. prepares input data\n",
    "        for processing, computes losses, and stores outputs in a dictionary.\n",
    "        :param batch: dictionary containing 'data', 'seg', etc.\n",
    "        :return: results_dict: dictionary with keys:\n",
    "                'boxes': list over batch elements. each batch element is a list of boxes. each box is a dictionary:\n",
    "                        [[{box_0}, ... {box_n}], [{box_0}, ... {box_n}], ...]\n",
    "                'seg_preds': pixel-wise class predictions (b, 1, y, x, (z)) with values [0, n_classes].\n",
    "                'monitor_values': dict of values to be monitored.\n",
    "        \"\"\"\n",
    "        img = batch['data']\n",
    "        gt_class_ids = batch['roi_labels']\n",
    "        gt_boxes = batch['bb_target']\n",
    "        axes = (0, 2, 3, 1) if self.cf.dim == 2 else (0, 2, 3, 4, 1)\n",
    "        gt_masks = [np.transpose(batch['roi_masks'][ii], axes=axes) for ii in range(len(batch['roi_masks']))]\n",
    "\n",
    "\n",
    "        img = torch.from_numpy(img).float().cuda()\n",
    "        batch_rpn_class_loss = torch.FloatTensor([0]).cuda()\n",
    "        batch_rpn_bbox_loss = torch.FloatTensor([0]).cuda()\n",
    "\n",
    "        # list of output boxes for monitoring/plotting. each element is a list of boxes per batch element.\n",
    "        box_results_list = [[] for _ in range(img.shape[0])]\n",
    "\n",
    "        #forward passes. 1. general forward pass, where no activations are saved in second stage (for performance\n",
    "        # monitoring and loss sampling). 2. second stage forward pass of sampled rois with stored activations for backprop.\n",
    "        rpn_class_logits, rpn_pred_deltas, proposal_boxes, detections, detection_masks = self.forward(img)\n",
    "        mrcnn_class_logits, mrcnn_pred_deltas, mrcnn_pred_mask, target_class_ids, mrcnn_target_deltas, target_mask,  \\\n",
    "        sample_proposals = self.loss_samples_forward(gt_class_ids, gt_boxes, gt_masks)\n",
    "\n",
    "        # loop over batch\n",
    "        for b in range(img.shape[0]):\n",
    "            if len(gt_boxes[b]) > 0:\n",
    "\n",
    "                # add gt boxes to output list for monitoring.\n",
    "                for ix in range(len(gt_boxes[b])):\n",
    "                    box_results_list[b].append({'box_coords': batch['bb_target'][b][ix],\n",
    "                                                'box_label': batch['roi_labels'][b][ix], 'box_type': 'gt'})\n",
    "\n",
    "                # match gt boxes with anchors to generate targets for RPN losses.\n",
    "                rpn_match, rpn_target_deltas = mutils.gt_anchor_matching(self.cf, self.np_anchors, gt_boxes[b])\n",
    "\n",
    "                # add positive anchors used for loss to output list for monitoring.\n",
    "                pos_anchors = mutils.clip_boxes_numpy(self.np_anchors[np.argwhere(rpn_match == 1)][:, 0], img.shape[2:])\n",
    "                for p in pos_anchors:\n",
    "                    box_results_list[b].append({'box_coords': p, 'box_type': 'pos_anchor'})\n",
    "\n",
    "            else:\n",
    "                rpn_match = np.array([-1]*self.np_anchors.shape[0])\n",
    "                rpn_target_deltas = np.array([0])\n",
    "\n",
    "            rpn_match = torch.from_numpy(rpn_match).cuda()\n",
    "            rpn_target_deltas = torch.from_numpy(rpn_target_deltas).float().cuda()\n",
    "\n",
    "            # compute RPN losses.\n",
    "            rpn_class_loss, neg_anchor_ix = compute_rpn_class_loss(rpn_match, rpn_class_logits[b], self.cf.shem_poolsize)\n",
    "            rpn_bbox_loss = compute_rpn_bbox_loss(rpn_target_deltas, rpn_pred_deltas[b], rpn_match)\n",
    "            batch_rpn_class_loss += rpn_class_loss / img.shape[0]\n",
    "            batch_rpn_bbox_loss += rpn_bbox_loss / img.shape[0]\n",
    "\n",
    "            # add negative anchors used for loss to output list for monitoring.\n",
    "            neg_anchors = mutils.clip_boxes_numpy(self.np_anchors[np.argwhere(rpn_match == -1)][0, neg_anchor_ix], img.shape[2:])\n",
    "            for n in neg_anchors:\n",
    "                box_results_list[b].append({'box_coords': n, 'box_type': 'neg_anchor'})\n",
    "\n",
    "            # add highest scoring proposals to output list for monitoring.\n",
    "            rpn_proposals = proposal_boxes[b][proposal_boxes[b, :, -1].argsort()][::-1]\n",
    "            for r in rpn_proposals[:self.cf.n_plot_rpn_props, :-1]:\n",
    "                box_results_list[b].append({'box_coords': r, 'box_type': 'prop'})\n",
    "\n",
    "        # add positive and negative roi samples used for mrcnn losses to output list for monitoring.\n",
    "        if 0 not in sample_proposals.shape:\n",
    "            rois = mutils.clip_to_window(self.cf.window, sample_proposals).cpu().data.numpy()\n",
    "            for ix, r in enumerate(rois):\n",
    "                box_results_list[int(r[-1])].append({'box_coords': r[:-1] * self.cf.scale,\n",
    "                                            'box_type': 'pos_class' if target_class_ids[ix] > 0 else 'neg_class'})\n",
    "\n",
    "        batch_rpn_class_loss = batch_rpn_class_loss\n",
    "        batch_rpn_bbox_loss = batch_rpn_bbox_loss\n",
    "\n",
    "        # compute mrcnn losses.\n",
    "        mrcnn_class_loss = compute_mrcnn_class_loss(target_class_ids, mrcnn_class_logits)\n",
    "        mrcnn_bbox_loss = compute_mrcnn_bbox_loss(mrcnn_target_deltas, mrcnn_pred_deltas, target_class_ids)\n",
    "\n",
    "        # mrcnn can be run without pixelwise annotations available (Faster R-CNN mode).\n",
    "        # In this case, the mask_loss is taken out of training.\n",
    "        if not self.cf.frcnn_mode:\n",
    "            mrcnn_mask_loss = compute_mrcnn_mask_loss(target_mask, mrcnn_pred_mask, target_class_ids)\n",
    "        else:\n",
    "            mrcnn_mask_loss = torch.FloatTensor([0]).cuda()\n",
    "\n",
    "        loss = batch_rpn_class_loss + batch_rpn_bbox_loss + mrcnn_class_loss + mrcnn_bbox_loss + mrcnn_mask_loss\n",
    "\n",
    "        # monitor RPN performance: detection count = the number of correctly matched proposals per fg-class.\n",
    "        dcount = [list(target_class_ids.cpu().data.numpy()).count(c) for c in np.arange(self.cf.head_classes)[1:]]\n",
    "\n",
    "\n",
    "\n",
    "        # run unmolding of predictions for monitoring and merge all results to one dictionary.\n",
    "        return_masks = self.cf.return_masks_in_val if is_validation else False\n",
    "        results_dict = get_results(self.cf, img.shape, detections, detection_masks,\n",
    "                                   box_results_list, return_masks=return_masks)\n",
    "\n",
    "        results_dict['torch_loss'] = loss\n",
    "        results_dict['monitor_values'] = {'loss': loss.item(), 'class_loss': mrcnn_class_loss.item()}\n",
    "\n",
    "        results_dict['logger_string'] =  \\\n",
    "            \"loss: {0:.2f}, rpn_class: {1:.2f}, rpn_bbox: {2:.2f}, mrcnn_class: {3:.2f}, mrcnn_bbox: {4:.2f}, \" \\\n",
    "            \"mrcnn_mask: {5:.2f}, dcount {6}\".format(loss.item(), batch_rpn_class_loss.item(),\n",
    "                                                     batch_rpn_bbox_loss.item(), mrcnn_class_loss.item(),\n",
    "                                                     mrcnn_bbox_loss.item(), mrcnn_mask_loss.item(), dcount)\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "\n",
    "    def test_forward(self, batch, return_masks=True):\n",
    "        \"\"\"\n",
    "        test method. wrapper around forward pass of network without usage of any ground truth information.\n",
    "        prepares input data for processing and stores outputs in a dictionary.\n",
    "        :param batch: dictionary containing 'data'\n",
    "        :param return_masks: boolean. If True, full resolution masks are returned for all proposals (speed trade-off).\n",
    "        :return: results_dict: dictionary with keys:\n",
    "               'boxes': list over batch elements. each batch element is a list of boxes. each box is a dictionary:\n",
    "                       [[{box_0}, ... {box_n}], [{box_0}, ... {box_n}], ...]\n",
    "               'seg_preds': pixel-wise class predictions (b, 1, y, x, (z)) with values [0, n_classes]\n",
    "        \"\"\"\n",
    "        img = batch['data']\n",
    "        img = torch.from_numpy(img).float().cuda()\n",
    "        _, _, _, detections, detection_masks = self.forward(img)\n",
    "        results_dict = get_results(self.cf, img.shape, detections, detection_masks, return_masks=return_masks)\n",
    "        return results_dict\n",
    "\n",
    "\n",
    "    def forward(self, img, is_training=True):\n",
    "        \"\"\"\n",
    "        :param img: input images (b, c, y, x, (z)).\n",
    "        :return: rpn_pred_logits: (b, n_anchors, 2)\n",
    "        :return: rpn_pred_deltas: (b, n_anchors, (y, x, (z), log(h), log(w), (log(d))))\n",
    "        :return: batch_proposal_boxes: (b, n_proposals, (y1, x1, y2, x2, (z1), (z2), batch_ix)) only for monitoring/plotting.\n",
    "        :return: detections: (n_final_detections, (y1, x1, y2, x2, (z1), (z2), batch_ix, pred_class_id, pred_score)\n",
    "        :return: detection_masks: (n_final_detections, n_classes, y, x, (z)) raw molded masks as returned by mask-head.\n",
    "        \"\"\"\n",
    "        # extract features.\n",
    "        fpn_outs = self.fpn(img)\n",
    "        rpn_feature_maps = [fpn_outs[i] for i in self.cf.pyramid_levels]\n",
    "        self.mrcnn_feature_maps = rpn_feature_maps\n",
    "\n",
    "        # loop through pyramid layers and apply RPN.\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(self.rpn(p))\n",
    "\n",
    "        # concatenate layer outputs.\n",
    "        # convert from list of lists of level outputs to list of lists of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
    "        rpn_pred_logits, rpn_pred_probs, rpn_pred_deltas = outputs\n",
    "\n",
    "        # generate proposals: apply predicted deltas to anchors and filter by foreground scores from RPN classifier.\n",
    "        proposal_count = self.cf.post_nms_rois_training if is_training else self.cf.post_nms_rois_inference\n",
    "        batch_rpn_rois, batch_proposal_boxes = proposal_layer(rpn_pred_probs, rpn_pred_deltas, proposal_count, self.anchors, self.cf)\n",
    "\n",
    "        # merge batch dimension of proposals while storing allocation info in coordinate dimension.\n",
    "        batch_ixs = torch.from_numpy(np.repeat(np.arange(batch_rpn_rois.shape[0]), batch_rpn_rois.shape[1])).float().cuda()\n",
    "        rpn_rois = batch_rpn_rois.view(-1, batch_rpn_rois.shape[2])\n",
    "        self.rpn_rois_batch_info = torch.cat((rpn_rois, batch_ixs.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # this is the first of two forward passes in the second stage, where no activations are stored for backprop.\n",
    "        # here, all proposals are forwarded (with virtual_batch_size = batch_size * post_nms_rois.)\n",
    "        # for inference/monitoring as well as sampling of rois for the loss functions.\n",
    "        # processed in chunks of roi_chunk_size to re-adjust to gpu-memory.\n",
    "        chunked_rpn_rois = self.rpn_rois_batch_info.split(self.cf.roi_chunk_size)\n",
    "        class_logits_list, bboxes_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for chunk in chunked_rpn_rois:\n",
    "                chunk_class_logits, chunk_bboxes = self.classifier(self.mrcnn_feature_maps, chunk)\n",
    "                class_logits_list.append(chunk_class_logits)\n",
    "                bboxes_list.append(chunk_bboxes)\n",
    "        batch_mrcnn_class_logits = torch.cat(class_logits_list, 0)\n",
    "        batch_mrcnn_bbox = torch.cat(bboxes_list, 0)\n",
    "        self.batch_mrcnn_class_scores = F.softmax(batch_mrcnn_class_logits, dim=1)\n",
    "\n",
    "        # refine classified proposals, filter and return final detections.\n",
    "        detections = refine_detections(rpn_rois, self.batch_mrcnn_class_scores, batch_mrcnn_bbox, batch_ixs, self.cf, )\n",
    "\n",
    "        # forward remaining detections through mask-head to generate corresponding masks.\n",
    "        scale = [img.shape[2]] * 4 + [img.shape[-1]] * 2\n",
    "        scale = torch.from_numpy(np.array(scale[:self.cf.dim * 2] + [1])[None]).float().cuda()\n",
    "\n",
    "\n",
    "        detection_boxes = detections[:, :self.cf.dim * 2 + 1] / scale\n",
    "        with torch.no_grad():\n",
    "            detection_masks = self.mask(self.mrcnn_feature_maps, detection_boxes)\n",
    "\n",
    "        return [rpn_pred_logits, rpn_pred_deltas, batch_proposal_boxes, detections, detection_masks]\n",
    "\n",
    "\n",
    "    def loss_samples_forward(self, batch_gt_class_ids, batch_gt_boxes, batch_gt_masks):\n",
    "        \"\"\"\n",
    "        this is the second forward pass through the second stage (features from stage one are re-used).\n",
    "        samples few rois in detection_target_layer and forwards only those for loss computation.\n",
    "        :param batch_gt_class_ids: list over batch elements. Each element is a list over the corresponding roi target labels.\n",
    "        :param batch_gt_boxes: list over batch elements. Each element is a list over the corresponding roi target coordinates.\n",
    "        :param batch_gt_masks: list over batch elements. Each element is binary mask of shape (n_gt_rois, y, x, (z), c)\n",
    "        :return: sample_logits: (n_sampled_rois, n_classes) predicted class scores.\n",
    "        :return: sample_boxes: (n_sampled_rois, n_classes, 2 * dim) predicted corrections to be applied to proposals for refinement.\n",
    "        :return: sample_mask: (n_sampled_rois, n_classes, y, x, (z)) predicted masks per class and proposal.\n",
    "        :return: sample_target_class_ids: (n_sampled_rois) target class labels of sampled proposals.\n",
    "        :return: sample_target_deltas: (n_sampled_rois, 2 * dim) target deltas of sampled proposals for box refinement.\n",
    "        :return: sample_target_masks: (n_sampled_rois, y, x, (z)) target masks of sampled proposals.\n",
    "        :return: sample_proposals: (n_sampled_rois, 2 * dim) RPN output for sampled proposals. only for monitoring/plotting.\n",
    "        \"\"\"\n",
    "        # sample rois for loss and get corresponding targets for all Mask R-CNN head network losses.\n",
    "        sample_ix, sample_target_class_ids, sample_target_deltas, sample_target_mask = \\\n",
    "            detection_target_layer(self.rpn_rois_batch_info, self.batch_mrcnn_class_scores,\n",
    "                                   batch_gt_class_ids, batch_gt_boxes, batch_gt_masks, self.cf)\n",
    "\n",
    "        # re-use feature maps and RPN output from first forward pass.\n",
    "        sample_proposals = self.rpn_rois_batch_info[sample_ix]\n",
    "        if 0 not in sample_proposals.size():\n",
    "            sample_logits, sample_boxes = self.classifier(self.mrcnn_feature_maps, sample_proposals)\n",
    "            sample_mask = self.mask(self.mrcnn_feature_maps, sample_proposals)\n",
    "        else:\n",
    "            sample_logits = torch.FloatTensor().cuda()\n",
    "            sample_boxes = torch.FloatTensor().cuda()\n",
    "            sample_mask = torch.FloatTensor().cuda()\n",
    "\n",
    "        return [sample_logits, sample_boxes, sample_mask, sample_target_class_ids, sample_target_deltas,\n",
    "                sample_target_mask, sample_proposals]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
