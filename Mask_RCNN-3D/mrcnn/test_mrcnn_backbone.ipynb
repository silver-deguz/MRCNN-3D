{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.engine as KE\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.layers import Input, Lambda, Activation, Add, Concatenate\n",
    "\n",
    "from config_3D import Config3D\n",
    "from maskrcnn_3D import MaskRCNN3D\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "ANCHOR_MAX_IOU                 0.3\n",
      "ANCHOR_MIN_IOU                 0.7\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_SHAPES                [[32 32 32]\n",
      " [16 16 16]\n",
      " [ 8  8  8]\n",
      " [ 4  4  4]\n",
      " [ 2  2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     4\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.1 0.2 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "DIM                            3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                20\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              cube\n",
      "IMAGE_SHAPE                    [128 128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56, 56)\n",
      "NAME                           mrcnn\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.1 0.2 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MRCNN3D_Config(Config3D):\n",
    "    NAME = \"mrcnn\"\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "    NUM_CLASSES = 1 + 3\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "    \n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor size in pixels\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    VALIDATION_STEPS = 5\n",
    "\n",
    "config = MRCNN3D_Config()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing ResNet Backbone. Check if Tensor shapes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jdeguzman/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from resnet_3D import ResNet50, ResNet101\n",
    "\n",
    "ins = Input(shape=[None, None, None, config.IMAGE_SHAPE[3]],\n",
    "                        name=\"input_image\")\n",
    "outs = ResNet50(input_shape=ins, stage5=True, train_bn=True)\n",
    "model = Model(inputs=ins, outputs=outs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1c308fd358>,\n",
       " <keras.layers.convolutional.ZeroPadding3D at 0x1c308fd320>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c308fda20>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3091b0f0>,\n",
       " <keras.layers.core.Activation at 0x1c3091b080>,\n",
       " <keras.layers.pooling.MaxPooling3D at 0x1c30960f98>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c30935208>,\n",
       " <model_utils_3D.BatchNorm at 0x1c309c1be0>,\n",
       " <keras.layers.core.Activation at 0x1c30a6af98>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c30ae1fd0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c30b620b8>,\n",
       " <keras.layers.core.Activation at 0x1c30b0a438>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c30baa710>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c30c9ad30>,\n",
       " <model_utils_3D.BatchNorm at 0x1c30c27e80>,\n",
       " <model_utils_3D.BatchNorm at 0x1c30cfd5f8>,\n",
       " <keras.layers.merge.Add at 0x1c30d6a898>,\n",
       " <keras.layers.core.Activation at 0x1c30e414e0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c30e0fa20>,\n",
       " <model_utils_3D.BatchNorm at 0x1c30e88c18>,\n",
       " <keras.layers.core.Activation at 0x1c30ea78d0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c30f29f60>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3102f0b8>,\n",
       " <keras.layers.core.Activation at 0x1c30fd4438>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c310756d8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c310f1e80>,\n",
       " <keras.layers.merge.Add at 0x1c31191dd8>,\n",
       " <keras.layers.core.Activation at 0x1c311c95c0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c311ae1d0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c312345f8>,\n",
       " <keras.layers.core.Activation at 0x1c31234a20>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c312d5898>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31348b38>,\n",
       " <keras.layers.core.Activation at 0x1c31348a58>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3140fb00>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31490908>,\n",
       " <keras.layers.merge.Add at 0x1c31490940>,\n",
       " <keras.layers.core.Activation at 0x1c31567ef0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c31537ba8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c315cf160>,\n",
       " <keras.layers.core.Activation at 0x1c315cf9e8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3168da20>,\n",
       " <model_utils_3D.BatchNorm at 0x1c316edcf8>,\n",
       " <keras.layers.core.Activation at 0x1c316eda58>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c317bff60>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3182c080>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31809c50>,\n",
       " <model_utils_3D.BatchNorm at 0x1c319044a8>,\n",
       " <keras.layers.merge.Add at 0x1c31947978>,\n",
       " <keras.layers.core.Activation at 0x1c31a27eb8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c319eb9e8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31a8f048>,\n",
       " <keras.layers.core.Activation at 0x1c31a8f9b0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c31b4da20>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31ba8cf8>,\n",
       " <keras.layers.core.Activation at 0x1c31ba8a58>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c31c4be10>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31ccbc50>,\n",
       " <keras.layers.merge.Add at 0x1c31cee390>,\n",
       " <keras.layers.core.Activation at 0x1c31dc54a8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c31d909e8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31e09c18>,\n",
       " <keras.layers.core.Activation at 0x1c31e2a8d0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c31eab4e0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c31f4d780>,\n",
       " <keras.layers.core.Activation at 0x1c31f4d588>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3200bcc0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3206a748>,\n",
       " <keras.layers.merge.Add at 0x1c3206acf8>,\n",
       " <keras.layers.core.Activation at 0x1c32144518>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c31eabcc0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c321ae630>,\n",
       " <keras.layers.core.Activation at 0x1c321aeef0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32273f28>,\n",
       " <model_utils_3D.BatchNorm at 0x1c322ea908>,\n",
       " <keras.layers.core.Activation at 0x1c322ead30>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c323cc780>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32430a20>,\n",
       " <keras.layers.merge.Add at 0x1c32430780>,\n",
       " <keras.layers.core.Activation at 0x1c32507ef0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c324d69b0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3256b080>,\n",
       " <keras.layers.core.Activation at 0x1c3256b940>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32613c50>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32691860>,\n",
       " <keras.layers.core.Activation at 0x1c326917b8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32758cc0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c327d9320>,\n",
       " <model_utils_3D.BatchNorm at 0x1c327b5ba8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c328b0748>,\n",
       " <keras.layers.merge.Add at 0x1c328fb898>,\n",
       " <keras.layers.core.Activation at 0x1c329d34a8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c329ba438>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32a3e5f8>,\n",
       " <keras.layers.core.Activation at 0x1c32a3ef60>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32ad9fd0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32b74358>,\n",
       " <keras.layers.core.Activation at 0x1c32b74048>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32c3b898>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32c97748>,\n",
       " <keras.layers.merge.Add at 0x1c32c97cf8>,\n",
       " <keras.layers.core.Activation at 0x1c32db2cc0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32d55f28>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32ddb5f8>,\n",
       " <keras.layers.core.Activation at 0x1c32ddbf60>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32e77f98>,\n",
       " <model_utils_3D.BatchNorm at 0x1c32ef2e80>,\n",
       " <keras.layers.core.Activation at 0x1c32f174e0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c32fbdf60>,\n",
       " <model_utils_3D.BatchNorm at 0x1c33035f60>,\n",
       " <keras.layers.merge.Add at 0x1c33035dd8>,\n",
       " <keras.layers.core.Activation at 0x1c330f3ef0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c330d8e10>,\n",
       " <model_utils_3D.BatchNorm at 0x1c33179518>,\n",
       " <keras.layers.core.Activation at 0x1c33179e48>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33213d68>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3328de80>,\n",
       " <keras.layers.core.Activation at 0x1c332b8400>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33357f28>,\n",
       " <model_utils_3D.BatchNorm at 0x1c333d3fd0>,\n",
       " <keras.layers.merge.Add at 0x1c333d3d30>,\n",
       " <keras.layers.core.Activation at 0x1c33474fd0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33474da0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c335165c0>,\n",
       " <keras.layers.core.Activation at 0x1c33516f28>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c335b3f60>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3362ae80>,\n",
       " <keras.layers.core.Activation at 0x1c33655400>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c336f5ef0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3376ca90>,\n",
       " <keras.layers.merge.Add at 0x1c3376c9e8>,\n",
       " <keras.layers.core.Activation at 0x1c33867240>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3380ffd0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c338b1550>,\n",
       " <keras.layers.core.Activation at 0x1c338b1eb8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33969b70>,\n",
       " <model_utils_3D.BatchNorm at 0x1c339c7e10>,\n",
       " <keras.layers.core.Activation at 0x1c339f3390>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33a95e80>,\n",
       " <model_utils_3D.BatchNorm at 0x1c33ba0a58>,\n",
       " <keras.layers.merge.Add at 0x1c33ba09b0>,\n",
       " <keras.layers.core.Activation at 0x1c33c981d0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33cbbd68>,\n",
       " <model_utils_3D.BatchNorm at 0x1c33ce6518>,\n",
       " <keras.layers.core.Activation at 0x1c33ce6cf8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33d7eef0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c33dfee80>,\n",
       " <keras.layers.core.Activation at 0x1c33e23208>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33edfb70>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c33f3bbe0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c33f3bf60>,\n",
       " <model_utils_3D.BatchNorm at 0x1c34037198>,\n",
       " <keras.layers.merge.Add at 0x1c34058ef0>,\n",
       " <keras.layers.core.Activation at 0x1c3415dda0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3411c860>,\n",
       " <model_utils_3D.BatchNorm at 0x1c341a4cf8>,\n",
       " <keras.layers.core.Activation at 0x1c341c7828>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c34261dd8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c342dfef0>,\n",
       " <keras.layers.core.Activation at 0x1c3439eeb8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c343da668>,\n",
       " <model_utils_3D.BatchNorm at 0x1c344b41d0>,\n",
       " <keras.layers.merge.Add at 0x1c34457940>,\n",
       " <keras.layers.core.Activation at 0x1c3452fda0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c344fe860>,\n",
       " <model_utils_3D.BatchNorm at 0x1c34579cf8>,\n",
       " <keras.layers.core.Activation at 0x1c3459e828>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3463add8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c346b7e80>,\n",
       " <keras.layers.core.Activation at 0x1c3472dd68>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c347ad080>,\n",
       " <model_utils_3D.BatchNorm at 0x1c347f4908>,\n",
       " <keras.layers.merge.Add at 0x1c347f4710>,\n",
       " <keras.layers.core.Activation at 0x1c348ceda0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ResNet+FPN Backbone. Check if Tensor shapes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor 'ROI/strided_slice_109:0' shape=(5999,) dtype=int32>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1ad752db4bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaskrcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskRCNN3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./tests'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/maskrcnn_3D.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn_feat_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/maskrcnn_3D.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mnms_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRPN_NMS_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ROI\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             config=config)([rpn_class, rpn_bbox, anchors])\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/model_layers_3D.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m                         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnms_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGES_PER_GPU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                         names=[\"rpn_non_max_suppression\"])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/model_utils_3D.py\u001b[0m in \u001b[0;36mbatch_slice\u001b[0;34m(inputs, graph_fn, batch_size, names)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0minputs_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0moutput_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0moutput_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/model_layers_3D.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Non-maximum suppression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         proposals = batch_slice([boxes, scores],\n\u001b[0;32m--> 142\u001b[0;31m                         \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnms_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGES_PER_GPU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                         names=[\"rpn_non_max_suppression\"])\n",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/model_layers_3D.py\u001b[0m in \u001b[0;36mnms_proposals\u001b[0;34m(boxes, scores, proposal_count, nms_threshold)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnms_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m      \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m      \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m      \u001b[0;31m# Pad if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MRCNN-3D/Mask_RCNN-3D/mrcnn/model_utils_3D.py\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[0;34m(boxes, scores, max_output_size, threshold)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mpick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# Compute IoU of the picked box with the rest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mixs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mixs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;31m# Identify boxes with IoU over the threshold. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;31m# returns indices into ixs[1:], so add 1 to get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0m_check_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor 'ROI/strided_slice_109:0' shape=(5999,) dtype=int32>"
     ]
    }
   ],
   "source": [
    "maskrcnn = MaskRCNN3D('training', config, './tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskrcnn.keras_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskrcnn.keras_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskrcnn.compile(learning_rate = config.LEARNING_RATE, momentum=config.LEARNING_MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing 3D Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import utils_3D as utils\n",
    "import model_utils_3D as mutils\n",
    "\n",
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride, unif=False):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width, depth] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales_z =  scales #/ 2\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    if unif == True:\n",
    "        # Enumerate heights and widths and depths from scales and ratios\n",
    "        heights = scales / (ratios ** (1/3))\n",
    "        widths = scales * (ratios ** (1/3))\n",
    "        depths = scales * (ratios ** (2/3))\n",
    "\n",
    "        # Enumerate shifts in feature space\n",
    "        shifts_z = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "        shifts_y = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "        shifts_x = np.arange(0, shape[2], anchor_stride) * feature_stride\n",
    "        shifts_x, shifts_y, shifts_z = np.meshgrid(shifts_x, shifts_y, shifts_z)\n",
    "    \n",
    "    else:\n",
    "        # Enumerate heights and widths from scales and ratios\n",
    "        heights = scales / np.sqrt(ratios)\n",
    "        widths = scales * np.sqrt(ratios)\n",
    "        depths = np.tile(np.array(scales_z),\n",
    "                         len(ratios)//np.array(scales_z)[..., None].shape[0])\n",
    "                                  \n",
    "        # Enumerate shifts in feature space\n",
    "        feature_stride_z = feature_stride #/ 2\n",
    "        shifts_z = np.arange(0, shape[0], anchor_stride) * feature_stride_z\n",
    "        shifts_y = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "        shifts_x = np.arange(0, shape[2], anchor_stride) * feature_stride\n",
    "        shifts_x, shifts_y, shifts_z = np.meshgrid(shifts_x, shifts_y, shifts_z)\n",
    "                                  \n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "    box_depths, box_centers_z = np.meshgrid(depths, shifts_z)\n",
    "    \n",
    "    # Reshape to get a list of (z, y, x) and a list of (d, h, w)\n",
    "    box_centers = np.stack([box_centers_z, box_centers_y, box_centers_x], axis=2).reshape([-1, 3])\n",
    "    box_sizes = np.stack([box_depths, box_heights, box_widths], axis=2).reshape([-1, 3])\n",
    "\n",
    "    # Convert to corner coordinates (z1, y1, x1, z2, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes, box_centers\n",
    "\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides, anchor_stride, unif):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, z1, y2, x2, z2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, z1, y2, x2, z2)]\n",
    "    anchors = []\n",
    "    boxcenters = []\n",
    "    \n",
    "    for i in range(len(scales)):\n",
    "        a, bc = generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride, unif=unif)\n",
    "        anchors.append(a)\n",
    "        boxcenters.append(bc)\n",
    "    return np.concatenate(anchors, axis=0), np.concatenate(boxcenters, axis=0)\n",
    "\n",
    "\n",
    "def get_anchors(config, image_shape, uniform_method):\n",
    "    \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
    "    anchors, boxes = generate_pyramid_anchors(\n",
    "                config.RPN_ANCHOR_SCALES,\n",
    "                config.RPN_ANCHOR_RATIOS,\n",
    "                config.BACKBONE_SHAPES,\n",
    "                config.BACKBONE_STRIDES,\n",
    "                config.RPN_ANCHOR_STRIDE, uniform_method)\n",
    "    return anchors, boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anchors3D, boxcenters3D = get_anchors(config, config.IMAGE_SHAPE, uniform_method=False)\n",
    "print(anchors3D[:100,0])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(boxcenters3D[:,2], boxcenters3D[:,1], boxcenters3D[:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anchors3D, boxcenters3D = get_anchors(config, config.IMAGE_SHAPE, uniform_method=True)\n",
    "print(anchors3D[:100,0])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(boxcenters3D[:,2], boxcenters3D[:,1], boxcenters3D[:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Checking 2D Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import utils_3D as utils\n",
    "import model_utils_3D as mutils\n",
    "\n",
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    # Enumerate heights and widths and depths from scales and ratios\n",
    "    heights = scales / np.sqrt(ratios)\n",
    "    widths = scales * np.sqrt(ratios)\n",
    "\n",
    "    # Enumerate shifts in feature space\n",
    "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "\n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "    \n",
    "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
    "    box_centers = np.stack([box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
    "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
    "\n",
    "    # Convert to corner coordinates (z1, y1, x1, z2, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes, box_centers\n",
    "\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides, anchor_stride):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, z1, y2, x2, z2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, z1, y2, x2, z2)]\n",
    "    anchors = []\n",
    "    boxcenters = []\n",
    "    \n",
    "    for i in range(len(scales)):\n",
    "        a, bc = generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride)\n",
    "        anchors.append(a)\n",
    "        boxcenters.append(bc)\n",
    "\n",
    "    return np.concatenate(anchors, axis=0), np.concatenate(boxcenters, axis=0)\n",
    "\n",
    "\n",
    "def get_anchors(config):\n",
    "    \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
    "    anchors, boxes = generate_pyramid_anchors(\n",
    "                config['rpn_anchor_scales'],\n",
    "                config['rpn_anchor_ratios'],\n",
    "                config['backbone_shapes'],\n",
    "                config['backbone_strides'],\n",
    "                config['rpn_anchor_stride'])\n",
    "    return anchors, boxes\n",
    "\n",
    "config2D = {}\n",
    "config2D['image_shape'] = np.array([128, 128, 3])\n",
    "config2D['rpn_anchor_scales'] = (32, 64, 128, 256, 512)\n",
    "config2D['rpn_anchor_ratios'] = [0.5, 1, 2]\n",
    "config2D['rpn_anchor_stride'] = 1\n",
    "config2D['backbone_strides'] = [4, 8, 16, 32, 64]\n",
    "config2D['backbone_shapes'] = np.array([[int(math.ceil(config2D['image_shape'][0] / stride)),\n",
    "                                         int(math.ceil(config2D['image_shape'][1] / stride))]\n",
    "                                         for stride in config2D['backbone_strides']])\n",
    "anchors2D, boxcenters2D = get_anchors(config2D)\n",
    "\n",
    "print(anchors2D[:1000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(boxcenters2D[:,1], boxcenters2D[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing NMS 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, scores, max_output_size, threshold):\n",
    "    \"\"\"Performs non-maximum suppression and returns indices of kept boxes.\n",
    "    boxes: [N, (z1, y1, x1, z2, y2, x2)].\n",
    "    scores: 1-D tensor of box scores.\n",
    "    threshold: IoU threshold to use for filtering.\n",
    "    \"\"\"\n",
    "    assert boxes.shape[0] > 0\n",
    "\n",
    "    # Compute box volumes\n",
    "    z1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x1 = boxes[:, 2]\n",
    "    z2 = boxes[:, 3]\n",
    "    y2 = boxes[:, 4]\n",
    "    x2 = boxes[:, 5]\n",
    "    vol = (z2 - z1) * (y2 - y1) * (x2 - x1)\n",
    "    \n",
    "    # Get indicies of boxes sorted by scores (highest first)\n",
    "#     ixs = tf.nn.top_k(scores, boxes.shape[0], sorted=True,\n",
    "#                      name=\"top_anchors\").indices\n",
    "#     # ixs = tf.argsort(scores)[::-1]\n",
    "\n",
    "#     keep = []\n",
    "#     while ixs.shape[0] > 0:\n",
    "#         # Pick top box and add its index to the list\n",
    "#         i = ixs[0]\n",
    "#         keep.append(i)\n",
    "#         # Compute IoU of the picked box with the rest\n",
    "#         iou = compute_iou(boxes[i], boxes[ixs[1:]], vol[i], vol[ixs[1:]])\n",
    "#         # Identify boxes with IoU over the threshold. This\n",
    "#         # returns indices into ixs[1:], so add 1 to get\n",
    "#         # indices into ixs.\n",
    "#         remove_ixs = tf.where(iou > threshold)[0] + 1\n",
    "#         # Remove indices of the picked and overlapped boxes.\n",
    "#         ixs = tf.gather(ixs, remove_ixs)\n",
    "#         ixs = tf.gather(ixs, 0)\n",
    "#     return tf.constant(keep[:max_output_size])\n",
    "\n",
    "    ixs = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "\n",
    "    while ixs.shape[0] > 0:\n",
    "        # Pick top box and add its index to the list\n",
    "        i = ixs[0]\n",
    "        keep.append(i)\n",
    "        # Compute IoU of the picked box with the rest\n",
    "        iou = compute_iou(boxes[i], boxes[ixs[1:]], vol[i], vol[ixs[1:]])\n",
    "        # Identify boxes with IoU over the threshold. This\n",
    "        # returns indices into ixs[1:], so add 1 to get\n",
    "        # indices into ixs.\n",
    "        remove_ixs = np.where(iou > threshold)[0] + 1\n",
    "        # Remove indices of the picked and overlapped boxes.\n",
    "        ixs = np.delete(ixs, remove_ixs)\n",
    "        ixs = np.delete(ixs, 0)\n",
    "    return tf.constant(keep[:max_output_size]) #np.array(pick, dtype=np.int32)\n",
    "\n",
    "def compute_iou(box, boxes, box_vol, boxes_vol):\n",
    "    \"\"\"Calculates IoU of the given box with the array of the given boxes.\n",
    "    box: 1D vector [z1, y1, x1, z2, y2, x2]\n",
    "    boxes: [boxes_count, (z1, y1, x1, z2, y2, x2)]\n",
    "    box_vol: float. the volume of 'box'\n",
    "    boxes_vol: array of length boxes_count.\n",
    "\n",
    "    Note: the volumes are passed in rather than calculated here for\n",
    "    efficiency. Calculate once in the caller to avoid duplicate work.\n",
    "    \"\"\"\n",
    "    # Calculate intersection volumes\n",
    "    z1 = np.maximum(box[0], boxes[:, 0])\n",
    "    z2 = np.minimum(box[3], boxes[:, 3])\n",
    "    y1 = np.maximum(box[1], boxes[:, 1])\n",
    "    y2 = np.minimum(box[4], boxes[:, 4])\n",
    "    x1 = np.maximum(box[2], boxes[:, 2])\n",
    "    x2 = np.minimum(box[5], boxes[:, 5])\n",
    "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0) * np.maximum(z2 - z1, 0)\n",
    "    union = box_vol + boxes_vol[:] - intersection[:]\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "# Randomly generate boxes and scors\n",
    "boxes = np.random.uniform(-400,1200,(2000,6))\n",
    "print(boxes.shape)\n",
    "scores = np.random.uniform(0,1,2000)\n",
    "print(scores.shape)\n",
    "# print(\"raw proposals\\n\", boxes)\n",
    "\n",
    "# Clip to image size 800x800x800\n",
    "boxes[:, slice(0, 6, 2)] = np.clip(boxes[:, slice(0, 6, 2)], 0, 800)\n",
    "boxes[:, slice(1, 6, 2)] = np.clip(boxes[:, slice(1, 6, 2)], 0, 800)\n",
    "# print(\"clipped proposals\\n\", boxes)\n",
    "\n",
    "# Test NMS\n",
    "ixs = non_max_suppression(boxes, scores, max_output_size=300, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:  \n",
    "    print(ixs.shape)\n",
    "    print(ixs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
