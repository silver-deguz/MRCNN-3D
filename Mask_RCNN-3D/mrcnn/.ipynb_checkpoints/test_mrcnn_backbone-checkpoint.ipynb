{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.engine as KE\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.layers import Input, Lambda, Activation, Add, Concatenate\n",
    "\n",
    "from config_3D import Config3D\n",
    "from maskrcnn_3D import MaskRCNN3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "ANCHOR_MAX_IOU                 0.3\n",
      "ANCHOR_MIN_IOU                 0.7\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_SHAPES                [[32 32 32]\n",
      " [16 16 16]\n",
      " [ 8  8  8]\n",
      " [ 4  4  4]\n",
      " [ 2  2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     4\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.1 0.2 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "DIM                            3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                20\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              cube\n",
      "IMAGE_SHAPE                    [128 128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56, 56)\n",
      "NAME                           mrcnn\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.1 0.2 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MRCNN3D_Config(Config3D):\n",
    "    NAME = \"mrcnn\"\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "    NUM_CLASSES = 1 + 3\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "    \n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor size in pixels\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    VALIDATION_STEPS = 5\n",
    "\n",
    "config = MRCNN3D_Config()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing ResNet Backbone. Check if Tensor shapes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from resnet_3D import ResNet50, ResNet101\n",
    "\n",
    "ins = Input(shape=[None, None, None, config.IMAGE_SHAPE[3]],\n",
    "                        name=\"input_image\")\n",
    "outs = ResNet50(input_shape=ins, stage5=True, train_bn=True)\n",
    "model = Model(inputs=ins, outputs=outs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ResNet+FPN Backbone. Check if Tensor shapes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jdeguzman/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "maskrcnn = MaskRCNN3D('training', config, './tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x11060bbe0>,\n",
       " <keras.layers.convolutional.ZeroPadding3D at 0x1c3cb47898>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cb47198>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cb47a90>,\n",
       " <keras.layers.core.Activation at 0x1c3cb9ca58>,\n",
       " <keras.layers.pooling.MaxPooling3D at 0x1c3cbbde10>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cbd36a0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cc1cd68>,\n",
       " <keras.layers.core.Activation at 0x1c3cc1ccc0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x10f41ad68>,\n",
       " <model_utils_3D.BatchNorm at 0x10f44ec18>,\n",
       " <keras.layers.core.Activation at 0x10f435b38>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cc29c50>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cc45e48>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cc459e8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cc97908>,\n",
       " <keras.layers.merge.Add at 0x1c3ccd36a0>,\n",
       " <keras.layers.core.Activation at 0x1c3ccede10>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cced160>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cd09a58>,\n",
       " <keras.layers.core.Activation at 0x1c3cd09ef0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cd49f98>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cd60a90>,\n",
       " <keras.layers.core.Activation at 0x1c3cd79400>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cdb7ac8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cdd2d30>,\n",
       " <keras.layers.merge.Add at 0x1c3cdd2278>,\n",
       " <keras.layers.core.Activation at 0x1c3ce11b70>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3ce11f60>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3ce41400>,\n",
       " <keras.layers.core.Activation at 0x1c3ce41908>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3ce7ecc0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3ce9ae10>,\n",
       " <keras.layers.core.Activation at 0x1c3ce9aa58>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3ced5da0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3ceeecc0>,\n",
       " <keras.layers.merge.Add at 0x1c3cf066a0>,\n",
       " <keras.layers.core.Activation at 0x1c3cf45908>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cf453c8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cf63e10>,\n",
       " <keras.layers.core.Activation at 0x1c3cf63be0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3cf9feb8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3cc97e10>,\n",
       " <keras.layers.core.Activation at 0x1c3cfcb860>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d00fe80>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d02b860>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d02bc18>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d07cb70>,\n",
       " <keras.layers.merge.Add at 0x1c3d095e10>,\n",
       " <keras.layers.core.Activation at 0x1c3d0d6208>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d0d67f0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d0f4d30>,\n",
       " <keras.layers.core.Activation at 0x1c3d0f4550>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d12fc88>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d142da0>,\n",
       " <keras.layers.core.Activation at 0x1c3d15f780>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d19cda0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d1bab38>,\n",
       " <keras.layers.merge.Add at 0x1c3d1baf98>,\n",
       " <keras.layers.core.Activation at 0x1c3d1f7e80>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d1f72b0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d2257b8>,\n",
       " <keras.layers.core.Activation at 0x1c3d225ba8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d266f28>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d27fcf8>,\n",
       " <keras.layers.core.Activation at 0x1c3d27fa20>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d2bae48>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d2ec240>,\n",
       " <keras.layers.merge.Add at 0x1c3d2ec978>,\n",
       " <keras.layers.core.Activation at 0x1c3d32e5c0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d32eba8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d347a20>,\n",
       " <keras.layers.core.Activation at 0x1c3d384eb8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d398550>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d3b76a0>,\n",
       " <keras.layers.core.Activation at 0x1c3d3b7b00>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d3f4e48>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d40ceb8>,\n",
       " <keras.layers.merge.Add at 0x1c3d44beb8>,\n",
       " <keras.layers.core.Activation at 0x1c3d460e10>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d460128>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d47ca90>,\n",
       " <keras.layers.core.Activation at 0x1c3d47cf28>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d4bd898>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d4d5f98>,\n",
       " <keras.layers.core.Activation at 0x1c3d4eea58>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d52cb00>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d548cf8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d548898>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d59d7f0>,\n",
       " <keras.layers.merge.Add at 0x1c3d59df98>,\n",
       " <keras.layers.core.Activation at 0x1c3d5f3cf8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d5f33c8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d60e978>,\n",
       " <keras.layers.core.Activation at 0x1c3d60ee10>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d64cf28>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d67f9b0>,\n",
       " <keras.layers.core.Activation at 0x1c3d664be0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d6baa20>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d6da7b8>,\n",
       " <keras.layers.merge.Add at 0x1c3d6dac18>,\n",
       " <keras.layers.core.Activation at 0x1c3d715908>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d715a58>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d74a9b0>,\n",
       " <keras.layers.core.Activation at 0x1c3d74a828>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d785ba8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d7a2e10>,\n",
       " <keras.layers.core.Activation at 0x1c3d7a2358>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d7dfc18>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d7f5be0>,\n",
       " <keras.layers.merge.Add at 0x1c3d80e5c0>,\n",
       " <keras.layers.core.Activation at 0x1c3d84b240>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d84b828>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d868d30>,\n",
       " <keras.layers.core.Activation at 0x1c3d868b00>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d8a3dd8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d8bada0>,\n",
       " <keras.layers.core.Activation at 0x1c3d8d4780>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d914da0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d932b38>,\n",
       " <keras.layers.merge.Add at 0x1c3d932f98>,\n",
       " <keras.layers.core.Activation at 0x1c3d96ce80>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d96c2b0>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d99b748>,\n",
       " <keras.layers.core.Activation at 0x1c3d99bba8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3d9def60>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3d9f8cf8>,\n",
       " <keras.layers.core.Activation at 0x1c3d9f8a20>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3da31e48>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3da63240>,\n",
       " <keras.layers.merge.Add at 0x1c3da63978>,\n",
       " <keras.layers.core.Activation at 0x1c3daa5860>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3daa5b70>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3dabeda0>,\n",
       " <keras.layers.core.Activation at 0x1c3dafdeb8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3db0d550>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3db2c6d8>,\n",
       " <keras.layers.core.Activation at 0x1c3db2cb38>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3db68e48>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3db83eb8>,\n",
       " <keras.layers.merge.Add at 0x1c3db83be0>,\n",
       " <keras.layers.core.Activation at 0x1c3dbd7e10>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dbd7240>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3dbf4a90>,\n",
       " <keras.layers.core.Activation at 0x1c3dbf4f28>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dc33f98>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3dc49a90>,\n",
       " <keras.layers.core.Activation at 0x1c3dc67400>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dca2b00>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dcbecf8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3dcbe898>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3dd137f0>,\n",
       " <keras.layers.merge.Add at 0x1c3dd13f98>,\n",
       " <keras.layers.core.Activation at 0x1c3dd68390>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dd68438>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3dd849b0>,\n",
       " <keras.layers.core.Activation at 0x1c3dd84e10>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3ddc4f28>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3ddf89b0>,\n",
       " <keras.layers.core.Activation at 0x1c3dddcbe0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3de31a20>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3de4c7b8>,\n",
       " <keras.layers.merge.Add at 0x1c3de4cc18>,\n",
       " <keras.layers.core.Activation at 0x1c3de8ba58>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3de8b240>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3debe978>,\n",
       " <keras.layers.core.Activation at 0x1c3debe828>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3def9ba8>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3df17e10>,\n",
       " <keras.layers.core.Activation at 0x1c3df17358>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3df52c18>,\n",
       " <model_utils_3D.BatchNorm at 0x1c3df6abe0>,\n",
       " <keras.layers.merge.Add at 0x1c3df825c0>,\n",
       " <keras.layers.core.Activation at 0x1c3dfc3828>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dfc3160>,\n",
       " <keras.layers.convolutional.UpSampling3D at 0x1c3dfe05c0>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3dfe0eb8>,\n",
       " <keras.layers.merge.Add at 0x1c3dfe0d30>,\n",
       " <keras.layers.convolutional.UpSampling3D at 0x1c3e050940>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3e0506a0>,\n",
       " <keras.layers.merge.Add at 0x1c3e050320>,\n",
       " <keras.layers.convolutional.UpSampling3D at 0x1c3e0a5f98>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3e0a54e0>,\n",
       " <keras.layers.merge.Add at 0x1c3e0c4470>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3e1899e8>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3e0fdb00>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3e136780>,\n",
       " <keras.layers.convolutional.Conv3D at 0x1c3e16b400>,\n",
       " <keras.layers.pooling.MaxPooling3D at 0x1c3e1bc390>,\n",
       " <keras.engine.training.Model at 0x10dd5bb00>,\n",
       " <keras.layers.merge.Concatenate at 0x10dd3fa58>,\n",
       " <keras.layers.merge.Concatenate at 0x10f943e80>,\n",
       " <keras.engine.input_layer.InputLayer at 0x1c3cb47358>,\n",
       " <keras.engine.input_layer.InputLayer at 0x1c3cb47208>,\n",
       " <keras.layers.merge.Concatenate at 0x10dd718d0>,\n",
       " <keras.layers.core.Lambda at 0x10dd3f208>,\n",
       " <keras.layers.core.Lambda at 0x10dd3f2e8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskrcnn.keras_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rpn_class_logits/concat:0' shape=(?, ?, 2) dtype=float32>,\n",
       " <tf.Tensor 'rpn_class/concat:0' shape=(?, ?, 2) dtype=float32>,\n",
       " <tf.Tensor 'rpn_bbox/concat:0' shape=(?, ?, 6) dtype=float32>,\n",
       " <tf.Tensor 'rpn_class_loss/cond/Merge:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'rpn_bbox_loss/cond/Merge:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskrcnn.keras_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskrcnn.compile(learning_rate = config.LEARNING_RATE, momentum=config.LEARNING_MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing 3D Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import utils_3D as utils\n",
    "import model_utils_3D as mutils\n",
    "\n",
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride, unif=False):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width, depth] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales_z =  scales #/ 2\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    if unif == True:\n",
    "        # Enumerate heights and widths and depths from scales and ratios\n",
    "        heights = scales / (ratios ** (1/3))\n",
    "        widths = scales * (ratios ** (1/3))\n",
    "        depths = scales * (ratios ** (2/3))\n",
    "\n",
    "        # Enumerate shifts in feature space\n",
    "        shifts_z = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "        shifts_y = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "        shifts_x = np.arange(0, shape[2], anchor_stride) * feature_stride\n",
    "        shifts_x, shifts_y, shifts_z = np.meshgrid(shifts_x, shifts_y, shifts_z)\n",
    "    \n",
    "    else:\n",
    "        # Enumerate heights and widths from scales and ratios\n",
    "        heights = scales / np.sqrt(ratios)\n",
    "        widths = scales * np.sqrt(ratios)\n",
    "        depths = np.tile(np.array(scales_z),\n",
    "                         len(ratios)//np.array(scales_z)[..., None].shape[0])\n",
    "                                  \n",
    "        # Enumerate shifts in feature space\n",
    "        feature_stride_z = feature_stride #/ 2\n",
    "        shifts_z = np.arange(0, shape[0], anchor_stride) * feature_stride_z\n",
    "        shifts_y = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "        shifts_x = np.arange(0, shape[2], anchor_stride) * feature_stride\n",
    "        shifts_x, shifts_y, shifts_z = np.meshgrid(shifts_x, shifts_y, shifts_z)\n",
    "                                  \n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "    box_depths, box_centers_z = np.meshgrid(depths, shifts_z)\n",
    "    \n",
    "    # Reshape to get a list of (z, y, x) and a list of (d, h, w)\n",
    "    box_centers = np.stack([box_centers_z, box_centers_y, box_centers_x], axis=2).reshape([-1, 3])\n",
    "    box_sizes = np.stack([box_depths, box_heights, box_widths], axis=2).reshape([-1, 3])\n",
    "\n",
    "    # Convert to corner coordinates (z1, y1, x1, z2, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes, box_centers\n",
    "\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides, anchor_stride, unif):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, z1, y2, x2, z2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, z1, y2, x2, z2)]\n",
    "    anchors = []\n",
    "    boxcenters = []\n",
    "    \n",
    "    for i in range(len(scales)):\n",
    "        a, bc = generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride, unif=unif)\n",
    "        anchors.append(a)\n",
    "        boxcenters.append(bc)\n",
    "    return np.concatenate(anchors, axis=0), np.concatenate(boxcenters, axis=0)\n",
    "\n",
    "\n",
    "def get_anchors(config, image_shape, uniform_method):\n",
    "    \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
    "    anchors, boxes = generate_pyramid_anchors(\n",
    "                config.RPN_ANCHOR_SCALES,\n",
    "                config.RPN_ANCHOR_RATIOS,\n",
    "                config.BACKBONE_SHAPES,\n",
    "                config.BACKBONE_STRIDES,\n",
    "                config.RPN_ANCHOR_STRIDE, uniform_method)\n",
    "    return anchors, boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anchors3D, boxcenters3D = get_anchors(config, config.IMAGE_SHAPE, uniform_method=False)\n",
    "print(anchors3D[:100,0])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(boxcenters3D[:,2], boxcenters3D[:,1], boxcenters3D[:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "anchors3D, boxcenters3D = get_anchors(config, config.IMAGE_SHAPE, uniform_method=True)\n",
    "print(anchors3D[:100,0])\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(boxcenters3D[:,2], boxcenters3D[:,1], boxcenters3D[:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Checking 2D Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import utils_3D as utils\n",
    "import model_utils_3D as mutils\n",
    "\n",
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    # Enumerate heights and widths and depths from scales and ratios\n",
    "    heights = scales / np.sqrt(ratios)\n",
    "    widths = scales * np.sqrt(ratios)\n",
    "\n",
    "    # Enumerate shifts in feature space\n",
    "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "\n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "    \n",
    "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
    "    box_centers = np.stack([box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
    "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
    "\n",
    "    # Convert to corner coordinates (z1, y1, x1, z2, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes, box_centers\n",
    "\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides, anchor_stride):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, z1, y2, x2, z2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, z1, y2, x2, z2)]\n",
    "    anchors = []\n",
    "    boxcenters = []\n",
    "    \n",
    "    for i in range(len(scales)):\n",
    "        a, bc = generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride)\n",
    "        anchors.append(a)\n",
    "        boxcenters.append(bc)\n",
    "\n",
    "    return np.concatenate(anchors, axis=0), np.concatenate(boxcenters, axis=0)\n",
    "\n",
    "\n",
    "def get_anchors(config):\n",
    "    \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
    "    anchors, boxes = generate_pyramid_anchors(\n",
    "                config['rpn_anchor_scales'],\n",
    "                config['rpn_anchor_ratios'],\n",
    "                config['backbone_shapes'],\n",
    "                config['backbone_strides'],\n",
    "                config['rpn_anchor_stride'])\n",
    "    return anchors, boxes\n",
    "\n",
    "config2D = {}\n",
    "config2D['image_shape'] = np.array([128, 128, 3])\n",
    "config2D['rpn_anchor_scales'] = (32, 64, 128, 256, 512)\n",
    "config2D['rpn_anchor_ratios'] = [0.5, 1, 2]\n",
    "config2D['rpn_anchor_stride'] = 1\n",
    "config2D['backbone_strides'] = [4, 8, 16, 32, 64]\n",
    "config2D['backbone_shapes'] = np.array([[int(math.ceil(config2D['image_shape'][0] / stride)),\n",
    "                                         int(math.ceil(config2D['image_shape'][1] / stride))]\n",
    "                                         for stride in config2D['backbone_strides']])\n",
    "anchors2D, boxcenters2D = get_anchors(config2D)\n",
    "\n",
    "print(anchors2D[:1000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(boxcenters2D[:,1], boxcenters2D[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
